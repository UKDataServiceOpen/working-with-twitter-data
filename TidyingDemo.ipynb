{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-talk notes for Speaker!\n",
    "During talk:\n",
    "* Minimise file browser\n",
    "* move to this folder cd .\\Documents\\GitHub\\working-with-twitter-data\\\n",
    "* Zoom in\n",
    "* Clear cells\n",
    "* Share public link -> * Share public link -> https://github.com/UKDataServiceOpen/working-with-twitter-data/blob/main/TidyingDemo.ipynb\n",
    "\n",
    "Talk time - 20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twarc Tidying and Analysis\n",
    "This notebook will cover the exploration, tidying up and some basic analysis of the data collected by the [TwarcDemo in this repo](https://github.com/UKDataServiceOpen/working-with-twitter-data/blob/main/TwarcDemo.ipynb)\n",
    "\n",
    "We will be using the 1000 Vegan Tweets from \"Veganuary\" 2019. If I didn't show a Twarc demo before you can find these in `data/demoData.csv`, and if you didn't follow along there is a provided version of this data.\n",
    "\n",
    "So let's import some packages and read it in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Our data manipulation library\n",
    "import numpy as np # Support for matrices, and other table-like shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak default plotting styles\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams.update({'font.size': 22,\n",
    "                    'figure.figsize':(24,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data into a dataframe using pandas\n",
    "data = pd.read_csv('data/demoData.csv')\n",
    "\n",
    "# The head function prints out the first 5 rows.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our tweets are real by taking the ID from the first column and replacing the ID in any tweet we can find, which I will demo!\n",
    "\n",
    "So we've got our data read in successfully, let's print out some of the tweet text to make sure they have something to do with veganism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in [1,2,3,4,5]:\n",
    "    print(data['text'][index])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I always recommend running info() for basic type information.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and decribe() for statistical info.\n",
    "# data.describe()\n",
    "# Or to supress scientific notation\n",
    "data[['author.public_metrics.followers_count','public_metrics.like_count','public_metrics.retweet_count']].describe().apply(lambda s: s.apply('{0:.0f}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I am thinking we have too many columns to analyse really. It's worth at this point asking if there is anything we could remove now. Though if we are exploring this may be premature.\n",
    "\n",
    "So that looks good to me, We might notice there are some retweets in here. I personally prefer removing retweets and replies where possible as they complicate our analysis with duplicate and tweets that don't make sense in context.\n",
    "\n",
    "## Removing retweets and replies.\n",
    "We need a way of detecting these. You might notice retweets start with two \"RT\" retweet characters. There is also a column that might help. let's check out the type column.\n",
    "\n",
    "We can call the value_counts function on any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also quickly call a plot function on any of these generated dataframes or value_counts.\n",
    "data['type'].value_counts().plot(kind='bar')\n",
    "\n",
    "# Though we are missing lots of Tweets here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular tweets have a type \"null\"\n",
    "# We can use boolean indexing to select only the rows that match this null condition.\n",
    "len(data[data['type'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lets select all the tweets that are typed as null.\n",
    "data = data[data['type'].isnull()]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length is correct but lets check things look okay\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reindex as well\n",
    "data = data.reset_index()\n",
    "# and delete old index\n",
    "del data['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the first five tweets again\n",
    "for index in [1,2,3,4,5]:\n",
    "    print(data['text'][index])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annoyingly removing tweets marked with type retweet doesn't seem to get them all. This is one of those many things with the Twitter API I can't seem to find an answer to.\n",
    "\n",
    "Luckily they are prepended with RT, which we can scrape an remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tweets with RT string\n",
    "data = data[~data['text'].str.contains('RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, reset index and let's test again.\n",
    "data = data.reset_index()\n",
    "\n",
    "del data['index']\n",
    "\n",
    "# lets check the first five tweets again\n",
    "for index in [1,2,3,4,5]:\n",
    "    print(data['text'][index])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it's probably time to narrow down what we are doing, there is too much interesting information in this dataset.\n",
    "\n",
    "I am going to keep the following:\n",
    "* id - The Tweet ID\n",
    "* created_at - The time the tweet was created\n",
    "* text - the text that makes up a tweet\n",
    "* author.id - the author ID\n",
    "* author.created_at - when the users account was created\n",
    "* author.username - the Twitter users username\n",
    "* author.location - a self-defined location\n",
    "* author.public_metrics.followers_count - Number of followers a user has\n",
    "* geo.full_name - the full name describing a tweets geolocation\n",
    "* public_metrics.like_count - number of likes on this tweet\n",
    "* public_metrics.retweet_count - number of retweets on this tweer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['id','created_at', 'text','author.id','author.created_at', 'author.username','author.location','author.public_metrics.followers_count','geo.full_name','public_metrics.like_count','public_metrics.retweet_count']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is Veganism percieved on Twitter?\n",
    "In order to answer this question we need to introduce sentiment analysis. This is quite easy to do in Python as complicated as it sounds. As with many complicated things, somebody has written a package to make this easy for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK, the Natural Language package\n",
    "import nltk\n",
    "# Download the popular vader lexicon of words and sentiments.\n",
    "nltk.download([\n",
    "    \"vader_lexicon\",\n",
    "])\n",
    "\n",
    "# import the sentiment analyser.\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a new sentiment analyser.\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# And write a function we can pass to our pandas function\n",
    "def get_sentiment(string):\n",
    "    return sia.polarity_scores(string)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this package our sentiment scores are returned on a scale of -1 for fully negative, to +1 for fully positive.\n",
    "So our below sentence \"I love cats\" has a sentiment of 0.6, we high positive sentiment. Whereas \"I hate cats is lower in sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our sentiment package\n",
    "get_sentiment('I love cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our sentiment package\n",
    "get_sentiment('I hate cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our sentiment package\n",
    "get_sentiment('I am cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment by word demo function\n",
    "def sentiment_by_word(string):\n",
    "    for word in string.split(' '):\n",
    "        print(word + ' -- ' + str(get_sentiment(word)))\n",
    "\n",
    "sentiment_by_word('I love cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's apply this to our entire dataframe\n",
    "data['sentiment'] = data['text'].apply(get_sentiment)\n",
    "\n",
    "# print a few rows\n",
    "for index in [0,1,2,3,4]:\n",
    "    print('sentiment ' + str(data['sentiment'][index]))\n",
    "    print(data['text'][index])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we do with our sentiment scores\n",
    "To start with, let's find our highest sentiment tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='sentiment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topSentimentIndex = 156\n",
    "print(data['id'][topSentimentIndex])\n",
    "print(data['text'][topSentimentIndex])\n",
    "print(data['sentiment'][topSentimentIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_word(data['text'][topSentimentIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And how about the lowest sentiment?\n",
    "data.sort_values(by='sentiment', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottomSentimentIndex = 104\n",
    "print(data['id'][bottomSentimentIndex])\n",
    "print(data['text'][bottomSentimentIndex])\n",
    "print(data['sentiment'][bottomSentimentIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_word(data['text'][bottomSentimentIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also grab some neutral tweets.\n",
    "data[data['sentiment'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralIndex = 3\n",
    "print(data['id'][neutralIndex])\n",
    "print(data['text'][neutralIndex])\n",
    "print(data['sentiment'][neutralIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_word(data['text'][neutralIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often the users at each end of this spectrum are quite different, lets see what our tweeters look like in general\n",
    "data.sentiment.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance there are three different kinds of tweets here:\n",
    "1. Negative tweets, these are likely complaints from vegans or complaints about vegans.\n",
    "2. Neutral tweets, Most of the users appear to be neutral, this is usually a symptom that our sentiment analyser wasn't trained on the language it's predicting on, so is seeing words it's never classified before and tags them as neutral.\n",
    "3. Positive tweets, these appear in abundance in comparison to negativity. Could this be a sign of positivity, marketing, bias?\n",
    "\n",
    "On a hunch this grouping is quite naive. I could imagine a big difference in group 1:\n",
    "* People complaining about vegans in a hateful way\n",
    "* Vegans complaining about non-vegans in a hateful way\n",
    "* Vegans complaining about vegan difficulties.\n",
    "\n",
    "group 3 likely contains:\n",
    "* Inflated self-promotion from vegan business owners\n",
    "* Inflated promotion and feedback from large brands launching vegan products such as Greggs and the Vegan sausage roll.\n",
    "\n",
    "These are all much larger project ideas, we don't even have a level of condience that any of these users are vegan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most liked content\n",
    "We have access to likes and retweets, let's check out what the most liked content is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fairly familiar graph of likes being geometrically hard to gain, with outliers from \"viral\" tweets\n",
    "data['public_metrics.like_count'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we see similar with retweets, even harder to come by as an echo of messaging rather than approval.\n",
    "data['public_metrics.retweet_count'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And how about the most liked tweet?\n",
    "data.sort_values(by='public_metrics.like_count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostLikedIndex = 1012\n",
    "print(data['id'][mostLikedIndex])\n",
    "print(data['text'][mostLikedIndex])\n",
    "print(data['sentiment'][mostLikedIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_word(data['text'][mostLikedIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And how about the most retweeted?\n",
    "data.sort_values(by='public_metrics.retweet_count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostRetweetedIndex = 1012\n",
    "print(data['id'][mostRetweetedIndex])\n",
    "print(data['text'][mostRetweetedIndex])\n",
    "print(data['sentiment'][mostRetweetedIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most liked and most retweeted came from Boy George, there is a correlation there that we will look at later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the perception of Veganism change over time?\n",
    "This is a toy example of what we saw in the presentation. How do we percieve a rolling sentiment over time.\n",
    "\n",
    "Next let's sort this dataframe by date. Looking now we seem to have only a handful of minutes between our Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date\n",
    "# convert created at into a datetime object\n",
    "data['created_at'] =pd.to_datetime(data.created_at)\n",
    "# Sort our dataframe b dates\n",
    "data = data.sort_values(by='created_at',ascending=True)\n",
    "# reset index\n",
    "data = data.reset_index()\n",
    "del data['index']\n",
    "\n",
    "# print the head\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start with let's plot sentiment over index. Although this is linear, rather than time based it can be useful.\n",
    "data['sentiment'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As each tweet is relatively unconnected we can get quite erratic plots, some smoothin can help.\n",
    "data['sentiment'].rolling(30).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive tweets\n",
    "len(data[data['sentiment'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral tweets\n",
    "len(data[data['sentiment'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative tweets\n",
    "len(data[data['sentiment'] < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our tweets are vegan, some or neutral, and few are negative.\n",
    "\n",
    "Not much to see here, in the full veganuary dataset we see a kickoff of positively, a trend downward and then a celebration at the end of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our neutral tweets, generally introduce some high-sentiment content that our model doesn't understand yet, so maybe we should remove them.\n",
    "data[data.sentiment != 0]['sentiment'].rolling(30).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with realistic time axis\n",
    "data.plot(kind='scatter',x='created_at', y='sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are only looking at such a small sample it's hard to draw any conclusions from this data. Depending on what we follow this can be a very clear line that somewhat represents the sentiment of a topic over time. Diving into our neutral tweets to better classify could be a good next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does sentiment correlate with success?\n",
    "Now we have quantified sentiment, do high sentiment messages get engagement?\n",
    "Let's plot our:\n",
    "* follower counts\n",
    "* likes count\n",
    "* retweet count\n",
    "* sentiment \n",
    "\n",
    "And see what we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data[['author.public_metrics.followers_count','public_metrics.like_count','public_metrics.retweet_count','sentiment']].corr()\n",
    "corr.style.background_gradient(cmap ='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My hunch is that as so many results have neutral sentiment this is probably shifting our correlations quite heavily, let's remove them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data[['author.public_metrics.followers_count','public_metrics.like_count','public_metrics.retweet_count','sentiment']][data['sentiment'] != 0].corr()\n",
    "corr.style.background_gradient(cmap ='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the more followers a user has, the more likely their content is to be liked and retweeted. Follower count does not correlate with the sentiment of tweets though.\n",
    "\n",
    "Likes and retweets have a strong correlation, content that is likely to be retweeted is also likely to be liked.\n",
    "\n",
    "In this case sentiment doesn't seem to correlate with any of these features though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point, we have noticed that our neutral sentiment tweets are a bit of a missed oppurtunity. We understand sentiment generally, but do not understand the terms within our topic area. The word \"carnivore\" might be usual, but in vegan circles it can be used in disgust or even as an insult, our sentiment analyzer doesn't understand this.\n",
    "\n",
    "A good next step would be trying to figure out what these words are, but I will leave this here for now.\n",
    "\n",
    "## Future Work\n",
    "* Getting a full word count from tweets\n",
    "* Dealing with stop words, punctuation and hashtags\n",
    "* Removing duplicate words through case sensitivity, fuzzy matching and stemming\n",
    "* Making word clouds with [wordclouds.co.uk](https://www.wordclouds.co.uk/)\n",
    "* Classifying types of tweet into marketing, self-promotion and true oppinion.\n",
    "\n",
    "## Useful Links\n",
    "* Word cloud builder - [wordclouds.co.uk](https://www.wordclouds.co.uk/)\n",
    "* An intro to basic NLP and word clouds with WhatsApp data - [What can I do with WhatsApp?](https://towardsdatascience.com/what-can-i-do-with-whatsapp-661fc3cdd5c5)\n",
    "* Use machine learning to understand and leverage text. - [Solving 90% of NLP](https://www.kdnuggets.com/2019/01/solve-90-nlp-problems-step-by-step-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
