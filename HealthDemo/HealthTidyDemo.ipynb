{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-talk notes for Speaker!\n",
    "During talk:\n",
    "* Minimise file browser\n",
    "* move to this folder cd .\\Documents\\GitHub\\working-with-twitter-data\\HealthDemo\n",
    "* Zoom in\n",
    "* Clear cells\n",
    "* Share public link -> https://github.com/UKDataServiceOpen/working-with-twitter-data/blob/main/HealthDemo/HealthTidyDemo.ipynb\n",
    "* Share [binder link](https://mybinder.org/v2/gh/UKDataServiceOpen/working-with-twitter-data/HEAD?labpath=%2FHealthDemo%2FHealthTidyDemo.ipynb)\n",
    "\n",
    "TODO - Time this talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twarc Tidying and Analysis\n",
    "This notebook covers my initial exploration of 7000 tweets from October 2021 to January 2022, all located in the UK. These tweets were scraped on the 16th Feb 2022 and all contain the keywords:\n",
    "- cough\n",
    "- coughing\n",
    "- sneeze\n",
    "- sneezing\n",
    "- fatigue\n",
    "- headache\n",
    "\n",
    "Otherwise regarded as common Covid-19 symptoms.\n",
    "This data was collected using the [HealthTwarcDemo notebook in this repo](https://github.com/UKDataServiceOpen/working-with-twitter-data/HealthDemo/HealthTwarcDemo.ipync)\n",
    "\n",
    "Throughout this notebook we cover:\n",
    "- Initial exploration of a dataset from Twitter\n",
    "- Visualising the increase in term over time\n",
    "- Investigating connected symptoms with some entry-level Natural language processing\n",
    "- Building a wordcloud from these words\n",
    "\n",
    "So let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our pacakges\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Change the default style to be bigger, and clearer colored.\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 28,\n",
    "    'figure.figsize':(28,12)\n",
    "})\n",
    "\n",
    "# set seaborn style\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Create a cool UK Data Service color palette for our plots\n",
    "colors = ['#E03A6C', '#F5AD42', '#ECE64B', '#449858', '#43A6C6', '#6C2B76']\n",
    "palette = sns.set_palette(sns.color_palette(colors), n_colors=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you couldn't successfully scrape any new data in the [HealthTwarcDemo notebook in this repo](https://github.com/UKDataServiceOpen/working-with-twitter-data/HealthDemo/HealthTwarcDemo.ipync) I have included a dataset, [3monthCoughUK.csv](https://github.com/UKDataServiceOpen/working-with-twitter-data/HealthDemo/3monthCoughUK.csv), which you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data into a dataframe using pandas, convert our dates to datetime objects so our plots can use them!\n",
    "data = pd.read_csv('3monthCoughUK.csv', parse_dates=['created_at'])\n",
    "\n",
    "# The head function prints out the first 5 rows.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to check a Tweet\n",
    "We can grab the ID from the first column here, and replace the ID in any existing tweet.\n",
    "\n",
    "For example here is the URL for one of my tweets about this webinar -> https://twitter.com/JosephAllen1234/status/1493911499047419907\n",
    "That messy number at the end is the Tweet ID. \n",
    "https://twitter.com/JosephAllen1234/status/Replace_me_with_a_tweet_ID\n",
    "\n",
    "So the first row above, has tweet ID - 1476704208074289156\n",
    "\n",
    "So even pasting the below, will redirect to the correct user.\n",
    "https://twitter.com/JosephAllen1234/status/1476704208074289156\n",
    "\n",
    "We should at this redirect see some mention of covid or one of our symptoms. An unfortunate side effect of twitters search is a user called \"coughsneeze\" may have all their tweets returned in our search too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['author.username'].str.contains(\"cough\", case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking some Tweet text\n",
    "So we've got our data read in successfully, let's print out some of the tweet text to make sure they have something to do with veganism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 5 tweets text for visual inspection\n",
    "for index in [1,2,3,4,5]:\n",
    "    print(data['text'][index])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I always recommend running info() for basic type information.\n",
    "# Here we are looking for any weird types, or largely missing values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and decribe() for statistical info.\n",
    "# data.describe()\n",
    "# Or to supress scientific notation\n",
    "data[['author.public_metrics.followers_count','public_metrics.like_count','public_metrics.retweet_count']].describe().apply(lambda s: s.apply('{0:.0f}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I am thinking we have too many columns to analyse really. It's worth at this point asking if there is anything we could remove now. Though if we are exploring this may be premature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is far too much to analyze here, considering what I am here to look for really the text is the main thing I care about. You may wish to retain the author ID if you want to track somebody who reports symptoms over time for example.\n",
    "\n",
    "I am going to keep the following:\n",
    "* id - The Tweet ID\n",
    "* created_at - The time the tweet was created\n",
    "* text - the text that makes up a tweet\n",
    "* author.id - the author ID\n",
    "* author.created_at - when the users account was created\n",
    "* author.username - the Twitter users username\n",
    "* author.location - a self-defined location\n",
    "* author.public_metrics.followers_count - Number of followers a user has\n",
    "* geo.full_name - the full name describing a tweets geolocation\n",
    "* public_metrics.like_count - number of likes on this tweet\n",
    "* public_metrics.retweet_count - number of retweets on this tweer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['id','created_at', 'text','author.id','author.created_at', 'author.username','author.location','author.public_metrics.followers_count','geo.full_name','public_metrics.like_count','public_metrics.retweet_count']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have covid-19 symptoms grown?\n",
    "Let's start by checking if there is evidence in these tweets that COVID has grown in the UK over this period.\n",
    "\n",
    "Those of us still checking the news will know that in late November we had the Omicron strain develop. I have included a dataset \"covid_data\" which contains new case numbers in the UK from https://ourworldindata.org/explorers/coronavirus-data-explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in official covid data, convert dates\n",
    "covid = pd.read_csv('covid_data.csv', parse_dates=['date'])\n",
    "\n",
    "# Set the date as our index so our plotting libraries format them\n",
    "covid = covid.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=covid).set(title='Number of covid cases over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very upsetting and familiar winter graph for Covid-19. But is it replicated in our very small Twitter dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=data['created_at'].groupby(data.created_at.dt.date).count().rolling(10).mean()).set(title='Number of tweets containing covid symptoms in UK over time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort of, there seemed to be larger reporting of covid symptoms back in October that have not that wasn't represented with more cases.\n",
    "Do the Tweets reflect covid numbers, covid paranoia or both?\n",
    "\n",
    "There is no doubt there is a correlation between these datasets, but at this point we can't really tell why this happens. Social media is infamous for \"look at me\" behavior which adds a huge bias here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symtpoms over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go a step further though, we could for example try to break down counts per day for tweets which contain our named symptoms. This could help us seperate out omicron hype from the delta hype. Omicron sufferers generally seem to have more of a sneeze and headache illness than the coughs we previously assocaited with covid-19. We can try to visualize this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build columns to flag whether text contains our keywords\n",
    "data['has_cough'] = data.text.str.contains('cough') | data.text.str.contains('coughing')\n",
    "data['has_sneeze'] = data.text.str.contains('sneeze') | data.text.str.contains('sneezing')\n",
    "data['has_fatigue'] = data.text.str.contains('fatigue')\n",
    "data['has_headache'] = data.text.str.contains('headache')\n",
    "\n",
    "# fatigue, sneeze, sneezing, headache\n",
    "data[['has_cough','has_sneeze','has_fatigue','has_headache']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log out some tweets that should contain coughs\n",
    "for index in [1,2,3]:\n",
    "    print(data[data.has_cough == True].reset_index()['text'][index])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log out some tweets that should contain sneezes\n",
    "for index in [1,2,3]:\n",
    "    print(data[data.has_sneeze == True].reset_index()['text'][index])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log out some tweets that should contain fatigue\n",
    "for index in [1,2,3]:\n",
    "    print(data[data.has_fatigue == True].reset_index()['text'][index])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log out some tweets that should contain headache\n",
    "for index in [1,2,3]:\n",
    "    print(data[data.has_headache == True].reset_index()['text'][index])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2x2 plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(25, 18))\n",
    "max = data.groupby(data.created_at.dt.date).has_cough.sum().max()\n",
    "\n",
    "# set datasets for each plot\n",
    "axes[0,0].set_ylim(0, max)\n",
    "axes[0,0].set_title('Mentions of cough or coughing')\n",
    "axes[0,1].set_ylim(0, max)\n",
    "axes[0,1].set_title('Mentions of sneeze or sneezing')\n",
    "axes[1,0].set_ylim(0, max)\n",
    "axes[1,0].set_title('Mentions of fatigue')\n",
    "axes[1,1].set_ylim(0, max)\n",
    "axes[1,1].set_title('Mentions of headache')\n",
    "\n",
    "# set big title\n",
    "fig.suptitle('Covid symptoms mentioned on Twitter over time')\n",
    "\n",
    "# create lineplots\n",
    "sns.lineplot(ax=axes[0, 0], data=data.groupby(data.created_at.dt.date).has_cough.sum().rolling(10).mean())\n",
    "sns.lineplot(ax=axes[0, 1],  data=data.groupby(data.created_at.dt.date).has_sneeze.sum().rolling(10).mean())\n",
    "sns.lineplot(ax=axes[1, 0], data=data.groupby(data.created_at.dt.date).has_fatigue.sum().rolling(10).mean())\n",
    "sns.lineplot(ax=axes[1, 1],  data=data.groupby(data.created_at.dt.date).has_headache.sum().rolling(10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there seems to be a baseline of fatigue and sneezing that aren't really growing with covid cases.\n",
    "\n",
    "On the other hand coughing and headaches appear at a higher frequency, and also seem to surge just before the number of covid cases do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have we missed a symptom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next hunch comes from a tweet like these:\n",
    "\"Fever, shaking, fatigue, swelling under armpit, heart pounding. Was quite scary at one point mate. Starting to feel bit better now\"\n",
    "\"slept,slept,slept headache gone, aches and pains gone, cough gone\"\n",
    "\n",
    "Users seem to be reporting symptoms I wasn't looking for like:\n",
    "- sleeping\n",
    "- heart pounding\n",
    "- shaking\n",
    "- swelling\n",
    "\n",
    "To find these, it might be worth looking for words which appear with our terms. To begin lets simply take a word frequency count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our NLP library\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text by whitespace\n",
    "# drop any empty rows before tokenize\n",
    "# data['text'] = data['text'].dropna()\n",
    "wordlist = data['text'].str.cat(sep=' ')\n",
    "words = nltk.tokenize.word_tokenize(wordlist)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "wordCount = pd.DataFrame(word_dist.most_common(),\n",
    "                    columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are our top 10 words?\n",
    "wordCount.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our most common words largely contain punctuation and what are called in NLP \"stop words\", these are words that traditionally add no context to a sentence like I, you, a, the, but etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text by whitespace\n",
    "# drop any empty rows before tokenize\n",
    "# data['text'] = data['text'].dropna()\n",
    "wordlist = data['text'].str.cat(sep=' ')\n",
    "words = nltk.tokenize.word_tokenize(wordlist)\n",
    "\n",
    "# remove non alphanumeric characters\n",
    "new_words= [word for word in words if word.isalnum()]\n",
    "\n",
    "word_dist = nltk.FreqDist(new_words)\n",
    "wordCount = pd.DataFrame(word_dist.most_common(),\n",
    "                    columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are our top ten words, without non alphanumeric cahracters\n",
    "wordCount.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation is gone, now let's remove \"stopwords\". NLTK actually has a list of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords are the words that add 'nothing' to a sentence, let's remove them. NLTK can help here.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and print stop words for demos sake\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text by whitespace\n",
    "# drop any empty rows before tokenize\n",
    "# data['text'] = data['text'].dropna()\n",
    "wordlist = data['text'].str.cat(sep=' ')\n",
    "words = nltk.tokenize.word_tokenize(wordlist)\n",
    "\n",
    "# remove non alphanumeric characters\n",
    "new_words= [word for word in words if word.isalnum()]\n",
    "\n",
    "# remove stop words\n",
    "from stop_words import get_stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_new_words = [w for w in new_words if not w.lower() in stop_words]\n",
    "\n",
    "word_dist = nltk.FreqDist(filtered_new_words)\n",
    "wordCount = pd.DataFrame(word_dist.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log words without stop words\n",
    "wordCount.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have all of them here, but we do have a lot of them. I in particular has been skipped because we haven't lowercased all our words when we tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text by whitespace\n",
    "# we add in a lower() function\n",
    "wordlist = data['text'].str.lower().str.cat(sep=' ')\n",
    "words = nltk.tokenize.word_tokenize(wordlist)\n",
    "\n",
    "# remove non alphanumeric characters\n",
    "new_words= [word for word in words if word.isalnum()]\n",
    "\n",
    "# remove stop words\n",
    "from stop_words import get_stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_new_words = [w for w in new_words if not w.lower() in stop_words]\n",
    "\n",
    "word_dist = nltk.FreqDist(filtered_new_words)\n",
    "wordCount = pd.DataFrame(word_dist.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all words, to merge Cough and cough for example.\n",
    "wordCount.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally we are likely underreporting any word here that can be conjugated.\n",
    "Think words like:\n",
    "- swim\n",
    "- swam\n",
    "- swimming\n",
    "\n",
    "all refer to the \"stem\" swim. We will experience this with:\n",
    "- cough\n",
    "- coughed\n",
    "- coughing\n",
    "\n",
    "So it's worth merging these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steam all words where possible\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text by whitespace\n",
    "# we add in a lower() function\n",
    "wordlist = data['text'].str.lower().str.cat(sep=' ')\n",
    "words = nltk.tokenize.word_tokenize(wordlist)\n",
    "\n",
    "# remove non alphanumeric characters\n",
    "new_words= [word for word in words if word.isalnum()]\n",
    "\n",
    "# remove stop words\n",
    "from stop_words import get_stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_new_words = [w for w in new_words if not w.lower() in stop_words]\n",
    "stems = [ps.stem(word) for word in filtered_new_words if word.isalnum()]\n",
    "\n",
    "word_dist = nltk.FreqDist(stems)\n",
    "wordCount = pd.DataFrame(word_dist.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo stemming of fatigued\n",
    "ps.stem('fatigued')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 50 occuring stems\n",
    "wordCount.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some web language leaking in now:\n",
    "- http is the protocol used to request assets on the web\n",
    "- amp is a special character code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove web terminology\n",
    "wordCount = wordCount[wordCount.Word != 'http']\n",
    "wordCount = wordCount[wordCount.Word != 'amp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=wordCount.head(20), y= wordCount.head(30).Word, x = wordCount.head(30).Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this alone we can see some common terms:\n",
    "- cold\n",
    "- sore\n",
    "- throat\n",
    "- back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Wordclouds\n",
    "Looking at this list is great, but a wordcloud is always a fun, and easy addition once you are at this point!\n",
    "We can use our wordCount object to build the format it wants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare format wordcloud pacakge expects\n",
    "bag = wordCount[['Word','Frequency']]\n",
    "bag.col = ['words','counts']\n",
    "\n",
    "d = {}\n",
    "for a, x in bag.values:\n",
    "    d[a] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud()\n",
    "wordcloud.generate_from_frequencies(frequencies=d)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower max_font_size, change the maximum number of word and lighten the background:\n",
    "wordcloud = WordCloud(width=1000, height= 700, max_font_size=200, max_words=100, background_color=\"white\").generate_from_frequencies(d)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_mask = np.array(Image.open(\"mask.png\"))\n",
    "\n",
    "# lower max_font_size, change the maximum number of word and lighten the background:\n",
    "wordcloud = WordCloud(width=1000, height= 700, max_font_size=500, max_words=100, background_color=\"white\", mask=covid_mask).generate_from_frequencies(d)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most liked content\n",
    "We have access to likes and retweets, let's check out what the most liked content is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And how about the most liked tweet?\n",
    "data.sort_values(by='public_metrics.like_count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostLikedIndex = 1496\n",
    "print(data['id'][mostLikedIndex])\n",
    "print(data['text'][mostLikedIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And how about the most retweeted?\n",
    "data.sort_values(by='public_metrics.retweet_count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostRetweetedIndex = 1496\n",
    "print(data['id'][mostRetweetedIndex])\n",
    "print(data['text'][mostRetweetedIndex])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
