{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-talk notes for Speaker!\n",
    "* Delete Academic tier app\n",
    "* get backup API keys\n",
    "\n",
    "During talk:\n",
    "* Open a terminal tab, and put in side pane\n",
    "* move to this folder cd .\\Documents\\GitHub\\working-with-twitter-data\\\n",
    "* Open the [Twitter Developer dashboard](https://developer.twitter.com/en/portal/dashboard)\n",
    "* Zoom in\n",
    "* Share public link -> https://github.com/UKDataServiceOpen/working-with-twitter-data/blob/main/TwarcDemo.ipynb\n",
    "\n",
    "Talk time - 25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Archive Search\n",
    "In this notebook we detail my recommended route to collecting Twitter Archive data. Through the usual free, or professional API tiers you can only access at most the last 30 days of Twitter data. This is adequate if you want to start collecting and curating a Twitter dataset, but largely useless if you want to look at the history of Twitter.\n",
    "\n",
    "Twitters archive search was released as part of Twitter's V2 API in August 2020, as such it's relatively new and largely unsupported by open source packages.\n",
    "Twitters documentation, and community tutorials have also changed a lot in this time.\n",
    "\n",
    "There are a few packages that can play with this V2 API, but my personal recommendation is [Twarc](https://github.com/DocNow/twarc). There are also a few plugins we can use to convert the output into a CSV file, and various other helpers.\n",
    "\n",
    "This package is a little weird as it runs as a command-line tool. It can be used as a Python package but in this regard the documentation is less clear currently.\n",
    "As this is a command-line tool, it's a little tricky to demo so most code cells here will exist to be copied into the terminal, and details for that installation are outlined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "You will need Python 3 and pip3 availible on your local machine. I recommend doing this by installing [Anaconda](https://docs.anaconda.com/anaconda/install/index.html)\n",
    "\n",
    "We can check these exist by typing the following:\n",
    "```\n",
    "python\n",
    "```\n",
    "which should open a REPL and print out our python version.\n",
    "And:\n",
    "```\n",
    "pip3\n",
    "```\n",
    "which should log out the manual for pip3. If these don't happen you will need to install these [here](https://www.python.org/downloads/) which may take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Twarc\n",
    "First check if you already have twarc installed:\n",
    "```\n",
    "twarc\n",
    "```\n",
    "You should see twarc is not recognized.\n",
    "\n",
    "Let's install it with:\n",
    "```\n",
    "pip3 install twarc\n",
    "```\n",
    "\n",
    "OR by running the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install twarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twarc time!\n",
    "Now we should have twarc installed, running `twarc` again will log out all the options we have.\n",
    "\n",
    "What we actually need is `twarc2` which supports the new V2 API, which has academic access! If you've got Python 3 you should be able to run this and see you already have twarc2. If you cannot run twarc2 follow the details for your OS [On the Twarc2 installation page](https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration & API keys\n",
    "Now comes the annoying bit. To talk to a web-based data source we need API keys so Twitter knows who is asking for millions of their tweets. I suggest not being to concerned with how this part works, we are here to collect our data and get out!\n",
    "\n",
    "## Twitter Developer Portal\n",
    "Before using twarc you will need to create an application and attach it to an project on your [Twitter Developer Portal](https://developer.twitter.com/en/portal/dashboard).\n",
    "\n",
    "1. Create an App\n",
    "2. Note down your API key, API key secret and Bearer token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING - copy the bearer in full\n",
    "bearer = ''\n",
    "api_key = ''\n",
    "api_key_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run\n",
    "```\n",
    "twarc2 configure\n",
    "```\n",
    "\n",
    "Follow the instructions:\n",
    "1. Enter your bearer token\n",
    "2. say y to optional user mode authentication\n",
    "3. Enter API and API secret\n",
    "4. generate access keys by visiting Twitter\n",
    "5. Enter pin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Twarc\n",
    "As a test to see if this is working run the following:\n",
    "```\n",
    "twarc2 search --archive --limit 100 cough 100cough.jsonl\n",
    "```\n",
    "We will build up to this, and more shortly.\n",
    "\n",
    "so let's break that request down:\n",
    "* twarc2 - use twarc with the V2 API\n",
    "* search - use the Twitter search endpoint\n",
    "* --archive - make an archive search\n",
    "* --limit 10 - only give me 10 tweets back\n",
    "* cough - our search term\n",
    "* 100cough.jsonl- our output file\n",
    "\n",
    "# Dashboarding\n",
    "Each application has a dashboard so we can check how close we are to our limit, For academic proejcts we can collect 10,000,000 tweets a month.\n",
    "\n",
    "So two questions come to mind:\n",
    "1. Did we hit the archive?\n",
    "2. Have we actually got 100 tweets about coughs\n",
    "\n",
    "To answer either we need to look at the data. JSON isn't very readable so it's time to look at some Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read in the data we need to make use of some python packages\n",
    "import pandas as pd #pandas, our data manipulation library\n",
    "import numpy as np #numpy, to support matrix style tables.\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we read our JSON into a dataframe.\n",
    "data = json.load(open('100cough.jsonl'))\n",
    "df = pd.DataFrame(data[\"data\"])\n",
    "\n",
    "# check the last five rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter URL formats\n",
    "A good next step is to actually inspect some of these tweets and see what data we get.\n",
    "\n",
    "If we navigate to any individual tweet on Twitter, for example `https://twitter.com/JosephAllen1234/status/1448222861701832704`\n",
    "\n",
    "This number at the end is our tweet ID. if we paste any ID from our dataset there we can navigate to it.\n",
    "\n",
    "You'll also notice some nested JSON files, we haven't really flattened out this data fully.\n",
    "\n",
    "## Twarc convert to CSV\n",
    "We aren't the first people to want to flatten this JSON object, there is a twarc plugin to convert to CSV.\n",
    "\n",
    "First make sure twarc is up to date with:\n",
    "```\n",
    "pip3 install --upgrade twarc\n",
    "twarc2 configure\n",
    "```\n",
    "\n",
    "Then install the twarc-csv plugin:\n",
    "```\n",
    "pip3 install --upgrade twarc-csv\n",
    "```\n",
    "OR if running straight from anaconda run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade twarc-csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert with the following pattern:\n",
    "```\n",
    "twarc2 csv tweets.jsonl tweets.csv\n",
    "```\n",
    "renaming files where needed.\n",
    "\n",
    "We can also use the \"--no-inline-referenced-tweets\" flag to remove replies referencing other tweets.\n",
    "```\n",
    "twarc2 csv tweets.jsonl tweets.csv --no-inline-referenced-tweets\n",
    "```\n",
    "So lets convert our 100 tweets.\n",
    "\n",
    "```\n",
    "twarc2 csv 100cough.jsonl 100cough.csv --no-inline-referenced-tweets\n",
    "```\n",
    "\n",
    "Now we should be able to read this in directly with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('100cough.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out all columns we have access to.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study - Omicron September 2021\n",
    "It's not enough that we simply search all tweets, in fact we will very quickly hit our 10,000,000 tweet limit with this approach, adding a one month delay to our research.\n",
    "\n",
    "## How useful is 10,000,000 tweets?\n",
    "From my previous attempts at this collecting all tweets containing the word vegan in January 2019 returned 1.2 million tweets. This took 32 hours and resulted in a 4.2GB file.\n",
    "\n",
    "There are an estimated 500 million tweets uploaded every day.\n",
    "As such it's important to narrow the scope of your project, and focus on very specific time periods.\n",
    "Luckily for covid symptoms there seems to be less of a marketing push than there are for vegan products. When looking at covid tweets tagged with location there are only a few thousand tweets a month.\n",
    "\n",
    "## The query\n",
    "My main hypothesis is \"Do self-reported symptoms on Twitter, predict a covid wave?\".\n",
    "we need the following data:\n",
    "* tweet ID\n",
    "* tweet text\n",
    "* user names and IDs\n",
    "* Anything that will help us check locations\n",
    "\n",
    "Let's build this up piece by piece\n",
    "\n",
    "### Custom search term\n",
    "We simply write the term we are searching for, \"cough\", after the search keyword.\n",
    "\n",
    "WARNING - DO NOT RUN THIS WITHOUT --limit\n",
    "```\n",
    "twarc2 search cough cough.jsonl\n",
    "```\n",
    "\n",
    "Of course \"cough\" isn't the only word linked with covid cases. Coughing, coughed and cough all count. There are also other symptoms like fatigue, headaches, sneezing that are reported. There are loads of operators we can use to join these [here](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#order-of-operations). Specifically we have to add parentheses, otherwise any spaces will be assumed as an AND operator.\n",
    "\n",
    "WARNING - DO NOT RUN THIS WITHOUT --limit\n",
    "```\n",
    "twarc2 search \"(cough OR coughing OR sneeze OR sneezing OR headache OR fatigue)\" cough.jsonl\n",
    "```\n",
    "\n",
    "### Limit the search\n",
    "Unless you want to capture all tweets that match this search, you should limit your search. adding `--limit x` replacing x with the number of tweets you want will help you here. \n",
    "\n",
    "```\n",
    "twarc2 search --limit 100 \"(cough OR coughing OR sneeze OR sneezing OR headache OR fatigue)\" 100cough.jsonl\n",
    "```\n",
    "\n",
    "### Archive search\n",
    "The `--archive` flag lets Twitter know we want to access more than the last 7 days. You will only be allowed to do this with the academic tier, it won't fail otherwise it will just give you recent tweets instead.\n",
    "\n",
    "```\n",
    "twarc2 search --archive --limit 100 \"(cough OR coughing OR sneeze OR sneezing OR headache OR fatigue)\" 100.jsonl\n",
    "```\n",
    "\n",
    "### Start and End times\n",
    "Adding a `--start-time` and `--end-time` flag lets you set start and end times to your archive search. These require dates in the format YYYY-MM-DD.\n",
    "\n",
    "```\n",
    "twarc2 search --archive --limit 100 --start-time \"2021-09-01\" --end-time \"2022-01-01\" \"(cough OR coughing OR sneeze OR sneezing OR headache OR fatigue)\" 100Covid2021.jsonl\n",
    "```\n",
    "\n",
    "### Location search\n",
    "We can collect a users self-defined location which varies from pronouns, to memes to relevant data. I suggest using place_country here, others seem to be less reliable and you can't relaly verify where a tweet came from.\n",
    "\n",
    "```\n",
    "twarc2 search --archive --limit 100 --start-time \"2021-09-01\" --end-time \"2022-01-01\" \"(vegan OR vegetarian OR plant-based) place_country:GB\" 100Covid2021.jsonl\n",
    "```\n",
    "\n",
    "So that's everything we need. There are some additional rules you can use based on your API tier [here](https://developer.twitter.com/en/docs/twitter-api/premium/rules-and-filtering/operators-by-product)\n",
    "\n",
    "Let's run this one last time to collect our data. remove the limit if you want the full dataset, though I won't run it to save some time.\n",
    "```\n",
    "twarc2 search --archive --limit 1000 --start-time \"2021-09-01\" --end-time \"2022-01-01\" \"(cough OR coughing OR fatigue OR sneeze OR sneezing OR headache) place_country:GB\" 3monthCovid2021.jsonl\n",
    "```\n",
    "Then convert it to a csv\n",
    "```\n",
    "twarc2 csv 3monthCovid2021.jsonl 3monthCovid2021.csv --no-inline-referenced-tweets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "* If you want to play with some data check out my [Tidying tutorial](https://github.com/UKDataServiceOpen/working-with-twitter-data/HealthDemo/HealthTidyingDemo.ipynb)\n",
    "* This [Twarc tutorial](https://github.com/alblaine/twarc-tutorial) breaks down installing Python, API keys and beyond!\n",
    "* This [Twarc2 tutorial](https://github.com/jeffcsauer/twarc-v2-tutorials/blob/master/twarc_fas.md) specifically covers archive searching.\n",
    "* The [Twarc report plugin](https://github.com/pbinkley/twarc-report) automates a lot of the tidying and visualization we would do in a next step.\n",
    "* The Twarc community is incredibly helpfulm if you have any issues or questions open them on their [GitHub](https://github.com/DocNow/twarc/issues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
